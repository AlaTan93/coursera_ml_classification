{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bedfeef-4c17-4105-ab4e-c3241202c533",
   "metadata": {},
   "source": [
    "# Cancer Prediction Lab for Supervised Machine Learning: Classification\n",
    "\n",
    "Notebook Author: Tan Song Xin Alastair\n",
    "\n",
    "Dataset Source: Kaggle\n",
    "\n",
    "Dataset Source URL: https://www.kaggle.com/datasets/rabieelkharoua/cancer-prediction-dataset\n",
    "\n",
    "Accessed Date: 02 February 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316618b-f138-4653-b461-ae27364bccca",
   "metadata": {},
   "source": [
    "## Pip Requirements:\n",
    "\n",
    "### Python Version: 3.13.1\n",
    "\n",
    "anyio==4.8.0\n",
    "\n",
    "argon2-cffi==23.1.0\n",
    "\n",
    "argon2-cffi-bindings==21.2.0\n",
    "\n",
    "arrow==1.3.0\n",
    "\n",
    "asttokens==3.0.0\n",
    "\n",
    "async-lru==2.0.4\n",
    "\n",
    "attrs==25.1.0\n",
    "\n",
    "babel==2.17.0\n",
    "\n",
    "beautifulsoup4==4.12.3\n",
    "\n",
    "bleach==6.2.0\n",
    "\n",
    "certifi==2025.1.31\n",
    "\n",
    "cffi==1.17.1\n",
    "\n",
    "charset-normalizer==3.4.1\n",
    "\n",
    "comm==0.2.2\n",
    "\n",
    "contourpy==1.3.1\n",
    "\n",
    "cycler==0.12.1\n",
    "\n",
    "debugpy==1.8.12\n",
    "\n",
    "decorator==5.1.1\n",
    "\n",
    "defusedxml==0.7.1\n",
    "\n",
    "executing==2.2.0\n",
    "\n",
    "fastjsonschema==2.21.1\n",
    "\n",
    "fonttools==4.55.8\n",
    "\n",
    "fqdn==1.5.1\n",
    "\n",
    "h11==0.14.0\n",
    "\n",
    "httpcore==1.0.7\n",
    "\n",
    "httpx==0.28.1\n",
    "\n",
    "idna==3.10\n",
    "\n",
    "ipykernel==6.29.5\n",
    "\n",
    "ipython==8.32.0\n",
    "\n",
    "ipywidgets==8.1.5\n",
    "\n",
    "isoduration==20.11.0\n",
    "\n",
    "jedi==0.19.2\n",
    "\n",
    "Jinja2==3.1.5\n",
    "\n",
    "joblib==1.4.2\n",
    "\n",
    "json5==0.10.0\n",
    "\n",
    "jsonpointer==3.0.0\n",
    "\n",
    "jsonschema==4.23.0\n",
    "\n",
    "jsonschema-specifications==2024.10.1\n",
    "\n",
    "jupyter==1.1.1\n",
    "\n",
    "jupyter-console==6.6.3\n",
    "\n",
    "jupyter-events==0.11.0\n",
    "\n",
    "jupyter-lsp==2.2.5\n",
    "\n",
    "jupyter_client==8.6.3\n",
    "\n",
    "jupyter_core==5.7.2\n",
    "\n",
    "jupyter_server==2.15.0\n",
    "\n",
    "jupyter_server_terminals==0.5.3\n",
    "\n",
    "jupyterlab==4.3.5\n",
    "\n",
    "jupyterlab_pygments==0.3.0\n",
    "\n",
    "jupyterlab_server==2.27.3\n",
    "\n",
    "jupyterlab_widgets==3.0.13\n",
    "\n",
    "kiwisolver==1.4.8\n",
    "\n",
    "MarkupSafe==3.0.2\n",
    "\n",
    "matplotlib==3.10.0\n",
    "\n",
    "matplotlib-inline==0.1.7\n",
    "\n",
    "mistune==3.1.1\n",
    "\n",
    "nbclient==0.10.2\n",
    "\n",
    "nbconvert==7.16.6\n",
    "\n",
    "nbformat==5.10.4\n",
    "\n",
    "nest-asyncio==1.6.0\n",
    "\n",
    "notebook==7.3.2\n",
    "\n",
    "notebook_shim==0.2.4\n",
    "\n",
    "numpy==2.2.2\n",
    "\n",
    "overrides==7.7.0\n",
    "\n",
    "packaging==24.2\n",
    "\n",
    "pandas==2.2.3\n",
    "\n",
    "pandocfilters==1.5.1\n",
    "\n",
    "parso==0.8.4\n",
    "\n",
    "pexpect==4.9.0\n",
    "\n",
    "pillow==11.1.0\n",
    "\n",
    "platformdirs==4.3.6\n",
    "\n",
    "prometheus_client==0.21.1\n",
    "\n",
    "prompt_toolkit==3.0.50\n",
    "\n",
    "psutil==6.1.1\n",
    "\n",
    "ptyprocess==0.7.0\n",
    "\n",
    "pure_eval==0.2.3\n",
    "\n",
    "pycparser==2.22\n",
    "\n",
    "Pygments==2.19.1\n",
    "\n",
    "pyparsing==3.2.1\n",
    "\n",
    "python-dateutil==2.9.0.post0\n",
    "\n",
    "python-json-logger==3.2.1\n",
    "\n",
    "pytz==2025.1\n",
    "\n",
    "PyYAML==6.0.2\n",
    "\n",
    "pyzmq==26.2.1\n",
    "\n",
    "referencing==0.36.2\n",
    "\n",
    "requests==2.32.3\n",
    "\n",
    "rfc3339-validator==0.1.4\n",
    "\n",
    "rfc3986-validator==0.1.1\n",
    "\n",
    "rpds-py==0.22.3\n",
    "\n",
    "scikit-learn==1.6.1\n",
    "\n",
    "scipy==1.15.1\n",
    "\n",
    "seaborn==0.13.2\n",
    "\n",
    "Send2Trash==1.8.3\n",
    "\n",
    "\n",
    "setuptools==75.8.0\n",
    "\n",
    "six==1.17.0\n",
    "\n",
    "sniffio==1.3.1\n",
    "\n",
    "soupsieve==2.6\n",
    "\n",
    "stack-data==0.6.3\n",
    "\n",
    "terminado==0.18.1\n",
    "\n",
    "threadpoolctl==3.5.0\n",
    "\n",
    "tinycss2==1.4.0\n",
    "\n",
    "tornado==6.4.2\n",
    "\n",
    "traitlets==5.14.3\n",
    "\n",
    "types-python-dateutil==2.9.0.20241206\n",
    "\n",
    "tzdata==2025.1\n",
    "\n",
    "uri-template==1.3.0\n",
    "\n",
    "urllib3==2.3.0\n",
    "\n",
    "wcwidth==0.2.13\n",
    "\n",
    "webcolors==24.11.1\n",
    "\n",
    "webencodings==0.5.1\n",
    "\n",
    "websocket-client==1.8.0\n",
    "\n",
    "widgetsnbextension==4.0.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277bcf49-f010-451c-95f3-9f31003ae0ae",
   "metadata": {},
   "source": [
    "# Dataset Summary\n",
    "\n",
    "The dataset is a collection of anonymised patient data with statistics on their age, gender, and health characters and history such as their BMI, whether the patient is smoking, their genetic risk of cancer, their number of hours per week of physical activity, the number of units of alcohol consumed per week, whether the patient has had cancer before, and the label column, on whether they have been diagnosed with cancer.\n",
    "\n",
    "This is a Kaggle dataset structured for purposes of testing prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8a8f95-e75b-45ed-b084-818f2534e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn, statistics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scipy.stats import kstest\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b164095f-43ba-457c-9044-83bf59b8f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7314ee7a-2370-4d91-ae9e-9ff56e34bee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Check:\n",
      "   Age  Gender        BMI  Smoking  GeneticRisk  PhysicalActivity  \\\n",
      "0   58       1  16.085313        0            1          8.146251   \n",
      "1   71       0  30.828784        0            1          9.361630   \n",
      "2   48       1  38.785084        0            2          5.135179   \n",
      "3   34       0  30.040296        0            0          9.502792   \n",
      "4   62       1  35.479721        0            0          5.356890   \n",
      "\n",
      "   AlcoholIntake  CancerHistory  Diagnosis  \n",
      "0       4.148219              1          1  \n",
      "1       3.519683              0          0  \n",
      "2       4.728368              0          1  \n",
      "3       2.044636              0          0  \n",
      "4       3.309849              0          1  \n",
      "NA/NULL count:\n",
      "Age                 0\n",
      "Gender              0\n",
      "BMI                 0\n",
      "Smoking             0\n",
      "GeneticRisk         0\n",
      "PhysicalActivity    0\n",
      "AlcoholIntake       0\n",
      "CancerHistory       0\n",
      "Diagnosis           0\n",
      "dtype: int64\n",
      "Gender\n",
      "0         764\n",
      "1         736\n",
      "Name: count, dtype: int64\n",
      "GeneticRisk\n",
      "0              895\n",
      "1              447\n",
      "2              158\n",
      "Name: count, dtype: int64\n",
      "Smoking\n",
      "0          1096\n",
      "1           404\n",
      "Name: count, dtype: int64\n",
      "CancerHistory\n",
      "0                1284\n",
      "1                 216\n",
      "Name: count, dtype: int64\n",
      "Diagnosis\n",
      "0            943\n",
      "1            557\n",
      "Name: count, dtype: int64\n",
      "Diagnosis\n",
      "0            571\n",
      "1            193\n",
      "Name: count, dtype: int64\n",
      "Diagnosis\n",
      "0            372\n",
      "1            364\n",
      "Name: count, dtype: int64\n",
      "Accuracy of naive model that predicts everyone has cancer: 0.37133333333333335.\n",
      "F1 Score of naive model that predicts everyone has cancer: 0.5415653864851726.\n"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "pd_dataset = pd.read_csv(\"cancer_pred_dataset.csv\")\n",
    "\n",
    "# Check if file is read properly.\n",
    "print(\"DataFrame Check:\")\n",
    "print(pd_dataset.head())\n",
    "\n",
    "# Check if there are null/na values to deal with\n",
    "print(\"NA/NULL count:\")\n",
    "print(pd_dataset.isna().sum())\n",
    "\n",
    "print(pd_dataset[[\"Gender\"]].value_counts())\n",
    "print(pd_dataset[[\"GeneticRisk\"]].value_counts())\n",
    "print(pd_dataset[[\"Smoking\"]].value_counts())\n",
    "print(pd_dataset[[\"CancerHistory\"]].value_counts())\n",
    "print(pd_dataset[[\"Diagnosis\"]].value_counts())\n",
    "print(pd_dataset.query(\"Gender==0\")[[\"Diagnosis\"]].value_counts())\n",
    "print(pd_dataset.query(\"Gender==1\")[[\"Diagnosis\"]].value_counts())\n",
    "\n",
    "print(f\"Accuracy of naive model that predicts everyone has cancer: {557/1500}.\")\n",
    "precision = 557 / (943 + 557)\n",
    "recall = 1\n",
    "\n",
    "print(f\"F1 Score of naive model that predicts everyone has cancer: {2 * (precision * recall) / (precision + recall)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530eacd1-151d-47bc-9965-19b807d7e5b4",
   "metadata": {},
   "source": [
    "## Categorical Value Counts:\n",
    "\n",
    "|GeneticRisk| | |Smoking| | |CancerHistory| | |Diagnosis| | \n",
    "|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|0|895| |0|1096| |0|1284| |0|943|\n",
    "|1|447| |1|404| |1|216| |1|557|\n",
    "|2|158| | | | |  | | | | |\n",
    "\n",
    "Some imbalance, but stratification will be attempted as a first simple step solution to this issue first. Notably, women are more likely in this dataset to have cancer than men. There are no null or NaN values, implying the dataset is high quality in terms of cleanliness.\n",
    "\n",
    "Aim to beat Accuracy of 0.371 and F1 Score of 0.542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf61aee3-3d1c-4ef7-8d5a-9c6b752b38bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Statistics=0.07162216580865866, p-value=3.891956357447314e-07\n",
      "Log Age Statistics=0.09156205580510224, p-value=2.1516391261112486e-11\n",
      "Boxcox Age Statistics=0.07219479869679402, p-value=3.0371878232906383e-07\n",
      "Sqrt Age Statistics=0.07501624938587448, p-value=8.695030230524278e-08\n",
      "Robust Scaler Scaled Age Statistics=0.07162216580865877, p-value=3.8919563574471276e-07\n"
     ]
    }
   ],
   "source": [
    "#Basic Age Two-Sided Kolmogorov-Smirnov Test. p > 0.05 means p is likely normal\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"Age\"], 'norm', args=(pd_dataset[\"Age\"].mean(), pd_dataset[\"Age\"].std()))\n",
    "print(f\"Age Statistics={stat}, p-value={p}\")\n",
    "\n",
    "stat, p = kstest(np.log(pd_dataset[\"Age\"]), 'norm', args=(np.log(pd_dataset[\"Age\"]).mean(), np.log(pd_dataset[\"Age\"]).std()))\n",
    "print(f\"Log Age Statistics={stat}, p-value={p}\")\n",
    "\n",
    "boxcox_result, _ = scipy.stats.boxcox(pd_dataset[\"Age\"])\n",
    "\n",
    "stat, p = kstest(boxcox_result, 'norm', args=(boxcox_result.mean(), boxcox_result.std()))\n",
    "print(f\"Boxcox Age Statistics={stat}, p-value={p}\")\n",
    "\n",
    "stat, p = kstest(np.sqrt(pd_dataset[\"Age\"]), 'norm', args=(np.sqrt(pd_dataset[\"Age\"]).mean(), np.sqrt(pd_dataset[\"Age\"]).std()))\n",
    "print(f\"Sqrt Age Statistics={stat}, p-value={p}\")\n",
    "\n",
    "robust_scaler_age = RobustScaler()\n",
    "pd_dataset[\"Scaled_Age\"] = robust_scaler_age.fit_transform(pd_dataset[[\"Age\"]])\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"Scaled_Age\"], 'norm', args=(pd_dataset[\"Scaled_Age\"].mean(), pd_dataset[\"Scaled_Age\"].std()))\n",
    "print(f\"Robust Scaler Scaled Age Statistics={stat}, p-value={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226e9db-c15a-4028-87a0-07dd75b547a6",
   "metadata": {},
   "source": [
    "## Age Normalisation Result Attempts:\n",
    "\n",
    "Unmodified Age p-value=3.891956357447314e-07\n",
    "\n",
    "Log Age p-value=2.1516391261112486e-11\n",
    "\n",
    "Boxcox Age p-value=3.0371878232906383e-07\n",
    "\n",
    "Sqrt Age p-value=8.695030230524278e-08\n",
    "\n",
    "Robust Scaler Scaled Age p-value=3.8919563574471276e-07\n",
    "\n",
    "None of the values are greater than 0.05. For the purposes of Logistic Regression, will be using the robust scaler version, as it is more resilient to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5321aaad-8b44-4046-bc25-cd4240432cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMI Statistics=0.06083960549816192, p-value=2.8679341466106453e-05\n",
      "Log BMI Statistics=0.08150339501149417, p-value=4.0836749374851926e-09\n",
      "Boxcox BMI Statistics=0.06405942746565263, p-value=8.547553639087e-06\n",
      "Sqrt BMI Statistics=0.06825455305362982, p-value=1.607386546593516e-06\n",
      "Robust Scaler Scaled BMI Statistics=0.06083960549816192, p-value=2.8679341466106453e-05\n"
     ]
    }
   ],
   "source": [
    "#Basic BMI Two-Sided Kolmogorov-Smirnov Test. p > 0.05 means p is likely normal\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"BMI\"], 'norm', args=(pd_dataset[\"BMI\"].mean(), pd_dataset[\"BMI\"].std()))\n",
    "print(f\"BMI Statistics={stat}, p-value={p}\")\n",
    "\n",
    "stat, p = kstest(np.log(pd_dataset[\"BMI\"]), 'norm', args=(np.log(pd_dataset[\"BMI\"]).mean(), np.log(pd_dataset[\"BMI\"]).std()))\n",
    "print(f\"Log BMI Statistics={stat}, p-value={p}\")\n",
    "\n",
    "boxcox_result, _ = scipy.stats.boxcox(pd_dataset[\"BMI\"])\n",
    "\n",
    "stat, p = kstest(boxcox_result, 'norm', args=(boxcox_result.mean(), boxcox_result.std()))\n",
    "print(f\"Boxcox BMI Statistics={stat}, p-value={p}\")\n",
    "\n",
    "stat, p = kstest(np.sqrt(pd_dataset[\"BMI\"]), 'norm', args=(np.sqrt(pd_dataset[\"BMI\"]).mean(), np.sqrt(pd_dataset[\"BMI\"]).std()))\n",
    "print(f\"Sqrt BMI Statistics={stat}, p-value={p}\")\n",
    "\n",
    "robust_scaler_bmi = RobustScaler()\n",
    "pd_dataset[\"Scaled_BMI\"] = robust_scaler_bmi.fit_transform(pd_dataset[[\"BMI\"]])\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"Scaled_BMI\"], 'norm', args=(pd_dataset[\"Scaled_BMI\"].mean(), pd_dataset[\"Scaled_BMI\"].std()))\n",
    "print(f\"Robust Scaler Scaled BMI Statistics={stat}, p-value={p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707130c-1363-4f5e-ac00-5b4155f1e19a",
   "metadata": {},
   "source": [
    "## BMI Normalisation Result Attempts:\n",
    "\n",
    "BMI Statistics=0.06083960549816192, p-value=2.8679341466106453e-05\n",
    "\n",
    "Log BMI Statistics=0.08150339501149417, p-value=4.0836749374851926e-09\n",
    "\n",
    "Boxcox BMI Statistics=0.06405942746565263, p-value=8.547553639087e-06\n",
    "\n",
    "Sqrt BMI Statistics=0.06825455305362982, p-value=1.607386546593516e-06\n",
    "\n",
    "Robust Scaler Scaled BMI Statistics=0.06083960549816192, p-value=2.8679341466106453e-05\n",
    "\n",
    "None of the values are greater than 0.05. For the purposes of Logistic Regression, will be using the robust scaler version, as it is more resilient to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c916f7-ee09-4214-bd2a-290bf6454bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalActivity Statistics=0.06320157936067106, p-value=1.1873079156051877e-05\n",
      "Log PhysicalActivity Statistics=0.14901562666999968, p-value=1.5348360737987005e-29\n",
      "Boxcox PhysicalActivity Statistics=0.9981014559780864, p-value=0.0\n",
      "Sqrt PhysicalActivity Statistics=0.07240336217258803, p-value=2.7735304269668635e-07\n",
      "Robust Scaler Scaled PhysicalActivity Statistics=0.06320157936067106, p-value=1.1873079156051877e-05\n",
      "Boxcox Robust Scaler Scaled PhysicalActivity Statistics=0.06185621834371713, p-value=1.9702041957745945e-05\n"
     ]
    }
   ],
   "source": [
    "#Basic Physical Activity Two-Sided Kolmogorov-Smirnov Test. p > 0.05 means p is likely normal\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"PhysicalActivity\"], 'norm', args=(pd_dataset[\"PhysicalActivity\"].mean(), pd_dataset[\"PhysicalActivity\"].std()))\n",
    "print(f\"PhysicalActivity Statistics={stat}, p-value={p}\")\n",
    "\n",
    "stat, p = kstest(np.log(pd_dataset[\"PhysicalActivity\"]), 'norm', args=(np.log(pd_dataset[\"PhysicalActivity\"]).mean(), np.log(pd_dataset[\"PhysicalActivity\"]).std()))\n",
    "print(f\"Log PhysicalActivity Statistics={stat}, p-value={p}\")\n",
    "\n",
    "pd_dataset[\"Boxcox_PhysicalActivity\"], lambda_boxcox_phy = scipy.stats.boxcox(pd_dataset[\"PhysicalActivity\"])\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"Boxcox_PhysicalActivity\"], 'norm', args=(boxcox_result.mean(), boxcox_result.std()))\n",
    "print(f\"Boxcox PhysicalActivity Statistics={stat}, p-value={p}\")\n",
    "\n",
    "stat, p = kstest(np.sqrt(pd_dataset[\"PhysicalActivity\"]), 'norm', args=(np.sqrt(pd_dataset[\"PhysicalActivity\"]).mean(), np.sqrt(pd_dataset[\"PhysicalActivity\"]).std()))\n",
    "print(f\"Sqrt PhysicalActivity Statistics={stat}, p-value={p}\")\n",
    "\n",
    "robust_scaler_phy = RobustScaler()\n",
    "pd_dataset[\"Scaled_PhysicalActivity\"] = robust_scaler_phy.fit_transform(pd_dataset[[\"PhysicalActivity\"]])\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"Scaled_PhysicalActivity\"], 'norm', args=(pd_dataset[\"Scaled_PhysicalActivity\"].mean(), pd_dataset[\"Scaled_PhysicalActivity\"].std()))\n",
    "print(f\"Robust Scaler Scaled PhysicalActivity Statistics={stat}, p-value={p}\")\n",
    "\n",
    "# Scale boxcox\n",
    "robust_scaler_boxcox_phy = RobustScaler()\n",
    "pd_dataset[\"Scaled_Boxcox_PhysicalActivity\"] = robust_scaler_phy.fit_transform(pd_dataset[[\"Boxcox_PhysicalActivity\"]])\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"Scaled_Boxcox_PhysicalActivity\"], 'norm', args=(pd_dataset[\"Scaled_Boxcox_PhysicalActivity\"].mean(), pd_dataset[\"Scaled_Boxcox_PhysicalActivity\"].std()))\n",
    "print(f\"Boxcox Robust Scaler Scaled PhysicalActivity Statistics={stat}, p-value={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3878e9-3fe0-4cd5-b1f5-bacd071fb609",
   "metadata": {},
   "source": [
    "## PhysicalActivity Normalisation Result Attempts\n",
    "\n",
    "PhysicalActivity p-value=1.1873079156051877e-05\n",
    "\n",
    "Log PhysicalActivity p-value=1.5348360737987005e-29\n",
    "\n",
    "Boxcox PhysicalActivity p-value=1.9128977936562205e-05\n",
    "\n",
    "Sqrt PhysicalActivity p-value=2.7735304269668635e-07\n",
    "\n",
    "Robust Scaler Scaled PhysicalActivity p-value=1.1873079156051877e-05\n",
    "\n",
    "There is an improvement of the p-value when using Boxcox, so we will use Robust Scaler on the results scaled by boxcos to get the p-value of 1.9702041957745945e-05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ebc885-fb6b-4b35-a904-edae55d0e76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlcoholIntake Statistics=0.05897210324534585, p-value=5.624101360878484e-05\n",
      "Log AlcoholIntake Statistics=0.1573101740462859, p-value=6.894405513893202e-33\n",
      "Boxcox AlcoholIntake Statistics=0.999834969922996, p-value=0.0\n",
      "Sqrt AlcoholIntake Statistics=0.07346645003055041, p-value=1.738732116775778e-07\n",
      "Robust Scaler Scaled AlcoholIntake Statistics=0.05897210324534585, p-value=5.624101360878484e-05\n"
     ]
    }
   ],
   "source": [
    "# Basic Alcohol Intake Two-Sided Kolmogorov-Smirnov Test. p > 0.05 means p is likely normal\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"AlcoholIntake\"], 'norm', args=(pd_dataset[\"AlcoholIntake\"].mean(), pd_dataset[\"AlcoholIntake\"].std()))\n",
    "print(f\"AlcoholIntake Statistics={stat}, p-value={p}\")\n",
    "\n",
    "stat, p = kstest(np.log(pd_dataset[\"AlcoholIntake\"]), 'norm', args=(np.log(pd_dataset[\"AlcoholIntake\"]).mean(), np.log(pd_dataset[\"AlcoholIntake\"]).std()))\n",
    "print(f\"Log AlcoholIntake Statistics={stat}, p-value={p}\")\n",
    "\n",
    "pd_dataset[\"Boxcox_AlcoholIntake\"], _ = scipy.stats.boxcox(pd_dataset[\"AlcoholIntake\"])\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"Boxcox_AlcoholIntake\"], 'norm', args=(boxcox_result.mean(), boxcox_result.std()))\n",
    "print(f\"Boxcox AlcoholIntake Statistics={stat}, p-value={p}\")\n",
    "\n",
    "stat, p = kstest(np.sqrt(pd_dataset[\"AlcoholIntake\"]), 'norm', args=(np.sqrt(pd_dataset[\"AlcoholIntake\"]).mean(), np.sqrt(pd_dataset[\"AlcoholIntake\"]).std()))\n",
    "print(f\"Sqrt AlcoholIntake Statistics={stat}, p-value={p}\")\n",
    "\n",
    "robust_scaler_alc = RobustScaler()\n",
    "pd_dataset[\"Scaled_AlcoholIntake\"] = robust_scaler_alc.fit_transform(pd_dataset[[\"AlcoholIntake\"]])\n",
    "\n",
    "stat, p = kstest(pd_dataset[\"Scaled_AlcoholIntake\"], 'norm', args=(pd_dataset[\"Scaled_AlcoholIntake\"].mean(), pd_dataset[\"Scaled_AlcoholIntake\"].std()))\n",
    "print(f\"Robust Scaler Scaled AlcoholIntake Statistics={stat}, p-value={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa2849-010c-4bc2-b9d5-cc60ecfa234d",
   "metadata": {},
   "source": [
    "## AlcoholIntake Normalisation Result Attempts\n",
    "\n",
    "AlcoholIntake p-value=5.624101360878484e-05\n",
    "\n",
    "Log AlcoholIntake p-value=6.894405513893202e-33\n",
    "\n",
    "Boxcox AlcoholIntake p-value=1.1360139376053743e-262\n",
    "\n",
    "Sqrt AlcoholIntake p-value=1.738732116775778e-07\n",
    "\n",
    "Robust Scaler Scaled AlcoholIntake p-value=5.624101360878484e-05\n",
    "\n",
    "None of the values are greater than 0.05. For the purposes of Logistic Regression, will be using the robust scaler version, as it is more resilient to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec855e04-9f45-4be9-b964-fd472e1efb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Standard Scaler will be used on GeneticRisk to convert it to values between 0 and 1.\n",
    "\n",
    "standard_scaler_genetic = StandardScaler()\n",
    "\n",
    "pd_dataset[\"GeneticRisk\"] = standard_scaler_genetic.fit_transform(pd_dataset[[\"GeneticRisk\"]])\n",
    "\n",
    "# Get dummies will be used on gender. And they will be renamed for interpretabiliy's sake.\n",
    "\n",
    "pd_dataset = pd.get_dummies(pd_dataset, columns=[\"Gender\"])\n",
    "\n",
    "pd_dataset.rename(columns={\"Gender_0\" : \"Gender_Male\", \"Gender_1\" : \"Gender_Female\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0fc60-e594-442b-83dd-0abbf645d3ba",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification\n",
    "\n",
    "No one-hot encoding will be done on GeneticRisk as there is some relationship between 0, 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9995a9-d263-48a9-b405-a22d0d99acfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Logistic Regression model accuracy: 0.8666666666666667, F1 Score: 0.8113207547169812\n",
      "Scaled_Age coefficient: 1.348329439310833\n",
      "Gender_Male coefficient: 0.9683508451568882\n",
      "Gender_Female coefficient: 0.9736233027039473\n",
      "Scaled_BMI coefficient: 1.2599341489373848\n",
      "Scaled_Boxcox_PhysicalActivity coefficient: 1.1764590217258286\n",
      "Scaled_AlcoholIntake coefficient: 1.2833416156722883\n",
      "GeneticRisk coefficient: 1.003716085902707\n",
      "Smoking coefficient: 1.7485106497652243\n",
      "CancerHistory coefficient: 3.7105488282194896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "X = pd_dataset[[\"Scaled_Age\", \"Gender_Male\", \"Gender_Female\", \"Scaled_BMI\", \"Scaled_Boxcox_PhysicalActivity\", \"Scaled_AlcoholIntake\", \"GeneticRisk\", \"Smoking\", \"CancerHistory\"]]\n",
    "y = pd_dataset[[\"Diagnosis\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Basic Logistic Regression model accuracy: {acc}, F1 Score: {f1}\")\n",
    "\n",
    "for index in range(len(X.columns)):\n",
    "    print(f\"{X.columns[index]} coefficient: {np.abs(clf.coef_[0][index])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70a287-9687-40fe-b2ff-4072d0395048",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "Basic Logistic Regression model accuracy: 0.87, F1 Score: 0.8151658767772512\n",
    "\n",
    "Scaled_Age coefficient: 1.348329439310833\n",
    "\n",
    "Gender_Male coefficient: 0.9683508451568882\n",
    "\n",
    "Gender_Female coefficient: 0.9736233027039473\n",
    "\n",
    "Scaled_BMI coefficient: 1.2599341489373848\n",
    "\n",
    "Scaled_Boxcox_PhysicalActivity coefficient: 1.1764590217258286\n",
    "\n",
    "Scaled_AlcoholIntake coefficient: 1.2833416156722883\n",
    "\n",
    "GeneticRisk coefficient: 1.003716085902707\n",
    "\n",
    "Smoking coefficient: 1.7485106497652243\n",
    "\n",
    "CancerHistory coefficient: 3.7105488282194896\n",
    "\n",
    "Quite reasonably, CancerHistory has the greatest impact on the model's predictions. Females are slightly more likely to have cancer in this dataset than males, even though there are (slightly) more males in this dataset than females. Smoking, Age and Alcohol intake all next have the highest impact. The genetic risk impact is lower than expected, being only higher than gender.\n",
    "\n",
    "Stratified K-Fold will be used to get another overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a3582b-b573-4fd4-979e-ddd11010fa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Stratified K-Folds: Mean accuracy: 0.8513333333333334, Mean F1 Score: 0.7892168261182981\n",
      "Scaled_Age coefficient: 1.5114603754516618\n",
      "Gender_Male coefficient: 0.986768307923724\n",
      "Gender_Female coefficient: 0.9892179805229308\n",
      "Scaled_BMI coefficient: 1.3515557102419415\n",
      "Scaled_Boxcox_PhysicalActivity coefficient: 1.1298400099817125\n",
      "Scaled_AlcoholIntake coefficient: 1.3948561844521046\n",
      "GeneticRisk coefficient: 1.0058234238941828\n",
      "Smoking coefficient: 1.832735041256917\n",
      "CancerHistory coefficient: 3.8359681337882905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits = 5\n",
    "acc_mean = 0.0\n",
    "f1_mean = 0.0\n",
    "\n",
    "age_coef_mean = 0.0\n",
    "male_coef_mean = 0.0\n",
    "female_coef_mean = 0.0\n",
    "bmi_coef_mean = 0.0\n",
    "phyact_coef_mean = 0.0\n",
    "alcohol_coef_mean = 0.0\n",
    "genetic_coef_mean = 0.0\n",
    "smoking_coef_mean = 0.0\n",
    "history_coef_mean = 0.0\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "\n",
    "    y_pred = clf.predict(X.iloc[test_index])\n",
    "\n",
    "    acc = accuracy_score(y.iloc[test_index], y_pred)\n",
    "    acc_mean += acc\n",
    "    f1 = f1_score(y.iloc[test_index], y_pred)\n",
    "    f1_mean += f1\n",
    "    \n",
    "    age_coef_mean += np.abs(clf.coef_[0][0])\n",
    "    male_coef_mean += np.abs(clf.coef_[0][1])\n",
    "    female_coef_mean += np.abs(clf.coef_[0][2])\n",
    "    bmi_coef_mean += np.abs(clf.coef_[0][3])\n",
    "    phyact_coef_mean += np.abs(clf.coef_[0][4])\n",
    "    alcohol_coef_mean += np.abs(clf.coef_[0][5])\n",
    "    genetic_coef_mean += np.abs(clf.coef_[0][6])\n",
    "    smoking_coef_mean += np.abs(clf.coef_[0][7])\n",
    "    history_coef_mean += np.abs(clf.coef_[0][8])\n",
    "\n",
    "print(f\"{n_splits} Stratified K-Folds: Mean accuracy: {acc_mean/n_splits}, Mean F1 Score: {f1_mean/n_splits}\")\n",
    "\n",
    "print(f\"{X.columns[0]} coefficient: {age_coef_mean/n_splits}\")\n",
    "print(f\"{X.columns[1]} coefficient: {male_coef_mean/n_splits}\")\n",
    "print(f\"{X.columns[2]} coefficient: {female_coef_mean/n_splits}\")\n",
    "print(f\"{X.columns[3]} coefficient: {bmi_coef_mean/n_splits}\")\n",
    "print(f\"{X.columns[4]} coefficient: {phyact_coef_mean/n_splits}\")\n",
    "print(f\"{X.columns[5]} coefficient: {alcohol_coef_mean/n_splits}\")\n",
    "print(f\"{X.columns[6]} coefficient: {genetic_coef_mean/n_splits}\")\n",
    "print(f\"{X.columns[7]} coefficient: {smoking_coef_mean/n_splits}\")\n",
    "print(f\"{X.columns[8]} coefficient: {history_coef_mean/n_splits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69959cc3-6551-48b4-8403-4d1903815e38",
   "metadata": {},
   "source": [
    "# Evaluation of Logistic Regression (and Stratified K-Fold Logistic Regression)\n",
    "\n",
    "|Values|Stratified Model|5-Fold Stratified Mean Model|\n",
    "|-----|-----|-----|\n",
    "|Accuracy|0.870|0.851|\n",
    "|F1 Score|0.815|0.789|\n",
    "||||\n",
    "|Age Coefficient|1.348|1.511|\n",
    "|Male Coefficient|0.968|0.987|\n",
    "|Female Coefficient|0.974|0.989|\n",
    "|BMI Coefficient|1.260|1.352|\n",
    "|Physical Activity Coefficient|1.176|1.130|\n",
    "|Alcohol Intake Coefficient|1.283|1.395|\n",
    "|Genetic Risk Coefficient|1.004|1.006|\n",
    "|Smoking Coefficient|1.749|1.833|\n",
    "|Cancer History Coefficient|3.711|3.836|\n",
    "\n",
    "Both the basic Logistic Regression (and the stratified K-Fold accuracy and F1-score means) are greater than the naive accuracy score of 0.623 and the naive F1 Score of 0.772.\n",
    "\n",
    "The 5-Fold model places a greater emphasis on age, BMI, alcohol intake, smoking and cancer history, but less on physical impact. It also has a smaller difference between male and female gender coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370f001a-e4d1-43b3-a001-5bcdfc6fbc8d",
   "metadata": {},
   "source": [
    "## Decision Tree Model\n",
    "\n",
    "Iterations on decision tree models will be attempted. Max depth will be used as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28dfbea1-6214-4a06-8bfb-5b8fa639863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model with max depth \"1\" accuracy: 0.71, F1 Score: 0.4\n",
      "Decision Tree model with max depth \"2\" accuracy: 0.7733333333333333, F1 Score: 0.6136363636363636\n",
      "Decision Tree model with max depth \"3\" accuracy: 0.7633333333333333, F1 Score: 0.5798816568047337\n",
      "Decision Tree model with max depth \"4\" accuracy: 0.7666666666666667, F1 Score: 0.6276595744680851\n",
      "Decision Tree model with max depth \"5\" accuracy: 0.79, F1 Score: 0.64\n",
      "Decision Tree model with max depth \"6\" accuracy: 0.8266666666666667, F1 Score: 0.7547169811320755\n",
      "Decision Tree model with max depth \"7\" accuracy: 0.8633333333333333, F1 Score: 0.8056872037914692\n",
      "Decision Tree model with max depth \"8\" accuracy: 0.9066666666666666, F1 Score: 0.8691588785046729\n",
      "Decision Tree model with max depth \"9\" accuracy: 0.9133333333333333, F1 Score: 0.8796296296296297\n",
      "Decision Tree model with max depth \"10\" accuracy: 0.9133333333333333, F1 Score: 0.8807339449541285\n",
      "Decision Tree model with max depth \"11\" accuracy: 0.9, F1 Score: 0.8636363636363636\n",
      "Decision Tree model with max depth \"12\" accuracy: 0.89, F1 Score: 0.8506787330316742\n",
      "Decision Tree model with max depth \"13\" accuracy: 0.8866666666666667, F1 Score: 0.8468468468468469\n",
      "Decision Tree model with max depth \"14\" accuracy: 0.8833333333333333, F1 Score: 0.8430493273542601\n",
      "Decision Tree model with max depth \"15\" accuracy: 0.8933333333333333, F1 Score: 0.8558558558558559\n",
      "Decision Tree model with max depth \"16\" accuracy: 0.8933333333333333, F1 Score: 0.8558558558558559\n",
      "5 Stratified K-Folds for Decision Tree Classifier, Max Depth of 8: Mean accuracy: 0.8866666666666667, Mean F1 Score: 0.8365767629502173\n",
      "5 Stratified K-Folds for Decision Tree Classifier, Max Depth of 9: Mean accuracy: 0.8873333333333333, Mean F1 Score: 0.8382895805375592\n",
      "5 Stratified K-Folds for Decision Tree Classifier, Max Depth of 10: Mean accuracy: 0.8813333333333333, Mean F1 Score: 0.8312930701286865\n",
      "5 Stratified K-Folds for Decision Tree Classifier, Max Depth of 11: Mean accuracy: 0.8786666666666667, Mean F1 Score: 0.8284795924780968\n",
      "5 Stratified K-Folds for Decision Tree Classifier, Max Depth of 12: Mean accuracy: 0.874, Mean F1 Score: 0.8225858398906588\n",
      "5 Stratified K-Folds for Decision Tree Classifier, Max Depth of 13: Mean accuracy: 0.8720000000000001, Mean F1 Score: 0.820825173418499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "for i in range(1, 17):    \n",
    "    dec_tree = DecisionTreeClassifier(max_depth=i, random_state=1)\n",
    "    dec_tree.fit(X_train, y_train)\n",
    "    y_pred = dec_tree.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Decision Tree model with max depth \\\"{i}\\\" accuracy: {acc}, F1 Score: {f1}\")\n",
    "\n",
    "n_splits = 5\n",
    "acc_mean = 0.0\n",
    "f1_mean = 0.0\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    dec_tree = DecisionTreeClassifier(max_depth=8, random_state=1)\n",
    "    dec_tree.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    y_pred = dec_tree.predict(X.iloc[test_index])\n",
    "    \n",
    "    acc = accuracy_score(y.iloc[test_index], y_pred)\n",
    "    acc_mean += acc\n",
    "    \n",
    "    f1 = f1_score(y.iloc[test_index], y_pred)\n",
    "    f1_mean += f1\n",
    "\n",
    "print(f\"{n_splits} Stratified K-Folds for Decision Tree Classifier, Max Depth of 8: Mean accuracy: {acc_mean/n_splits}, Mean F1 Score: {f1_mean/n_splits}\")\n",
    "\n",
    "acc_mean = 0.0\n",
    "f1_mean = 0.0\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    dec_tree = DecisionTreeClassifier(max_depth=9, random_state=1)\n",
    "    dec_tree.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    y_pred = dec_tree.predict(X.iloc[test_index])\n",
    "    \n",
    "    acc = accuracy_score(y.iloc[test_index], y_pred)\n",
    "    acc_mean += acc\n",
    "    \n",
    "    f1 = f1_score(y.iloc[test_index], y_pred)\n",
    "    f1_mean += f1\n",
    "\n",
    "print(f\"{n_splits} Stratified K-Folds for Decision Tree Classifier, Max Depth of 9: Mean accuracy: {acc_mean/n_splits}, Mean F1 Score: {f1_mean/n_splits}\")\n",
    "\n",
    "acc_mean = 0.0\n",
    "f1_mean = 0.0\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    dec_tree = DecisionTreeClassifier(max_depth=10, random_state=1)\n",
    "    dec_tree.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    y_pred = dec_tree.predict(X.iloc[test_index])\n",
    "    \n",
    "    acc = accuracy_score(y.iloc[test_index], y_pred)\n",
    "    acc_mean += acc\n",
    "    \n",
    "    f1 = f1_score(y.iloc[test_index], y_pred)\n",
    "    f1_mean += f1\n",
    "\n",
    "print(f\"{n_splits} Stratified K-Folds for Decision Tree Classifier, Max Depth of 10: Mean accuracy: {acc_mean/n_splits}, Mean F1 Score: {f1_mean/n_splits}\")\n",
    "\n",
    "acc_mean = 0.0\n",
    "f1_mean = 0.0\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    dec_tree = DecisionTreeClassifier(max_depth=11, random_state=1)\n",
    "    dec_tree.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    y_pred = dec_tree.predict(X.iloc[test_index])\n",
    "    \n",
    "    acc = accuracy_score(y.iloc[test_index], y_pred)\n",
    "    acc_mean += acc\n",
    "    \n",
    "    f1 = f1_score(y.iloc[test_index], y_pred)\n",
    "    f1_mean += f1\n",
    "\n",
    "print(f\"{n_splits} Stratified K-Folds for Decision Tree Classifier, Max Depth of 11: Mean accuracy: {acc_mean/n_splits}, Mean F1 Score: {f1_mean/n_splits}\")\n",
    "\n",
    "acc_mean = 0.0\n",
    "f1_mean = 0.0\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    dec_tree = DecisionTreeClassifier(max_depth=12, random_state=1)\n",
    "    dec_tree.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    y_pred = dec_tree.predict(X.iloc[test_index])\n",
    "    \n",
    "    acc = accuracy_score(y.iloc[test_index], y_pred)\n",
    "    acc_mean += acc\n",
    "    \n",
    "    f1 = f1_score(y.iloc[test_index], y_pred)\n",
    "    f1_mean += f1\n",
    "\n",
    "print(f\"{n_splits} Stratified K-Folds for Decision Tree Classifier, Max Depth of 12: Mean accuracy: {acc_mean/n_splits}, Mean F1 Score: {f1_mean/n_splits}\")\n",
    "\n",
    "acc_mean = 0.0\n",
    "f1_mean = 0.0\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    dec_tree = DecisionTreeClassifier(max_depth=13, random_state=1)\n",
    "    dec_tree.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    y_pred = dec_tree.predict(X.iloc[test_index])\n",
    "    \n",
    "    acc = accuracy_score(y.iloc[test_index], y_pred)\n",
    "    acc_mean += acc\n",
    "    \n",
    "    f1 = f1_score(y.iloc[test_index], y_pred)\n",
    "    f1_mean += f1\n",
    "\n",
    "print(f\"{n_splits} Stratified K-Folds for Decision Tree Classifier, Max Depth of 13: Mean accuracy: {acc_mean/n_splits}, Mean F1 Score: {f1_mean/n_splits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddafb8-70b8-478f-bffc-14e08bfd13d5",
   "metadata": {},
   "source": [
    "## Decision Tree Evaluation\n",
    "\n",
    "Decision trees prefer to use greedy splits to maximise the purity/gini of the splits.\n",
    "\n",
    "|Model Type|Max Depth|Accuracy|F1 Score|\n",
    "|-----|-----|-----|-----|\n",
    "|Decision Tree|1|0.710|0.400|\n",
    "|Decision Tree|2|0.773|0.614|\n",
    "|Decision Tree|3|0.763|0.580|\n",
    "|Decision Tree|4|0.767|0.628|\n",
    "|Decision Tree|5|0.790|0.640|\n",
    "|Decision Tree|6|0.827|0.757|\n",
    "|Decision Tree|7|0.863|0.806|\n",
    "|Decision Tree|8|0.907|0.869|\n",
    "|Decision Tree|9|0.913|0.880|\n",
    "|Decision Tree|10|0.913|0.881|\n",
    "|Decision Tree|11|0.900|0.864|\n",
    "|Decision Tree|12|0.890|0.851|\n",
    "|Decision Tree|13|0.887|0.847|\n",
    "|Decision Tree|14|0.883|0.843|\n",
    "|Decision Tree|15|0.893|0.856|\n",
    "|Decision Tree|16|0.893|0.856|\n",
    "|5-Fold Decision Tree (Mean)|8|0.887|0.837|\n",
    "|5-Fold Decision Tree (Mean)|9|0.887|0.838|\n",
    "|5-Fold Decision Tree (Mean)|10|0.881|0.831|\n",
    "|5-Fold Decision Tree (Mean)|11|0.879|0.828|\n",
    "|5-Fold Decision Tree (Mean)|12|0.874|0.823|\n",
    "|5-Fold Decision Tree (Mean)|13|0.872|0.821|\n",
    "\n",
    "The basic decision tree has the best F1 at a max depth of 10, and shows no change in accuracy or F1 Score past a max depth of 15. For the 5-Fold Decision Tree, the best accuracy and F1 score comes at the max depth of 9.\n",
    "\n",
    "The accuracy and F1 scores of the decision trees are better than the Logistic Regression results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d6ba0-5d79-4e85-9104-939d1f36e4c8",
   "metadata": {},
   "source": [
    "## Random Forest Classifier Evaluation\n",
    "\n",
    "Random Forest classifiers are an ensemble model that uses multiple decision trees in a voting manner. This aims to replicate the features of the Decision Tree classifier, but taking the 'average' to minimise overfitting. This comes at the cost of interpretability, and training time.\n",
    "\n",
    "We will be using the max depths of 8, 9 and 10 discovered as the best hyperparameters for this particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5551fd43-ee98-4abf-9b69-ab8c9d6c7431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Random Forest Classifier (max depth 8) with n_estimators 2 model accuracy: 0.84, F1 Score: 0.7623762376237624\n",
      "Basic Random Forest Classifier (max depth 8) with n_estimators 5 model accuracy: 0.8766666666666667, F1 Score: 0.821256038647343\n",
      "Basic Random Forest Classifier (max depth 8) with n_estimators 10 model accuracy: 0.89, F1 Score: 0.8374384236453202\n",
      "Basic Random Forest Classifier (max depth 8) with n_estimators 50 model accuracy: 0.9133333333333333, F1 Score: 0.875\n",
      "Basic Random Forest Classifier (max depth 8) with n_estimators 100 model accuracy: 0.9333333333333333, F1 Score: 0.9074074074074074\n",
      "Basic Random Forest Classifier (max depth 8) with n_estimators 500 model accuracy: 0.9266666666666666, F1 Score: 0.897196261682243\n",
      "Basic Random Forest Classifier (max depth 8) with n_estimators 1000 model accuracy: 0.9233333333333333, F1 Score: 0.8909952606635071\n",
      "Basic Random Forest Classifier (max depth 8) with n_estimators 5000 model accuracy: 0.9266666666666666, F1 Score: 0.8962264150943396\n",
      "Basic Random Forest Classifier (max depth 8) with n_estimators 10000 model accuracy: 0.9266666666666666, F1 Score: 0.8962264150943396\n",
      "Basic Random Forest Classifier (max depth 9) with n_estimators 2 model accuracy: 0.8033333333333333, F1 Score: 0.7121951219512195\n",
      "Basic Random Forest Classifier (max depth 9) with n_estimators 5 model accuracy: 0.89, F1 Score: 0.8465116279069768\n",
      "Basic Random Forest Classifier (max depth 9) with n_estimators 10 model accuracy: 0.9133333333333333, F1 Score: 0.8773584905660378\n",
      "Basic Random Forest Classifier (max depth 9) with n_estimators 50 model accuracy: 0.9266666666666666, F1 Score: 0.897196261682243\n",
      "Basic Random Forest Classifier (max depth 9) with n_estimators 100 model accuracy: 0.9333333333333333, F1 Score: 0.9065420560747663\n",
      "Basic Random Forest Classifier (max depth 9) with n_estimators 500 model accuracy: 0.9366666666666666, F1 Score: 0.9124423963133641\n",
      "Basic Random Forest Classifier (max depth 9) with n_estimators 1000 model accuracy: 0.94, F1 Score: 0.9166666666666666\n",
      "Basic Random Forest Classifier (max depth 9) with n_estimators 5000 model accuracy: 0.9333333333333333, F1 Score: 0.9074074074074074\n",
      "Basic Random Forest Classifier (max depth 9) with n_estimators 10000 model accuracy: 0.9333333333333333, F1 Score: 0.9074074074074074\n",
      "Basic Random Forest Classifier (max depth 10) with n_estimators 2 model accuracy: 0.8266666666666667, F1 Score: 0.74\n",
      "Basic Random Forest Classifier (max depth 10) with n_estimators 5 model accuracy: 0.89, F1 Score: 0.8465116279069768\n",
      "Basic Random Forest Classifier (max depth 10) with n_estimators 10 model accuracy: 0.9133333333333333, F1 Score: 0.8773584905660378\n",
      "Basic Random Forest Classifier (max depth 10) with n_estimators 50 model accuracy: 0.9366666666666666, F1 Score: 0.9116279069767442\n",
      "Basic Random Forest Classifier (max depth 10) with n_estimators 100 model accuracy: 0.94, F1 Score: 0.9166666666666666\n",
      "Basic Random Forest Classifier (max depth 10) with n_estimators 500 model accuracy: 0.9366666666666666, F1 Score: 0.9124423963133641\n",
      "Basic Random Forest Classifier (max depth 10) with n_estimators 1000 model accuracy: 0.9366666666666666, F1 Score: 0.9124423963133641\n",
      "Basic Random Forest Classifier (max depth 10) with n_estimators 5000 model accuracy: 0.9366666666666666, F1 Score: 0.9124423963133641\n",
      "Basic Random Forest Classifier (max depth 10) with n_estimators 10000 model accuracy: 0.9366666666666666, F1 Score: 0.9124423963133641\n",
      "Basic Random Forest Classifier (max depth None) with n_estimators 2 model accuracy: 0.8133333333333334, F1 Score: 0.6818181818181818\n",
      "Basic Random Forest Classifier (max depth None) with n_estimators 5 model accuracy: 0.8866666666666667, F1 Score: 0.8333333333333334\n",
      "Basic Random Forest Classifier (max depth None) with n_estimators 10 model accuracy: 0.8966666666666666, F1 Score: 0.848780487804878\n",
      "Basic Random Forest Classifier (max depth None) with n_estimators 50 model accuracy: 0.92, F1 Score: 0.8878504672897196\n",
      "Basic Random Forest Classifier (max depth None) with n_estimators 100 model accuracy: 0.9366666666666666, F1 Score: 0.9116279069767442\n",
      "Basic Random Forest Classifier (max depth None) with n_estimators 500 model accuracy: 0.9333333333333333, F1 Score: 0.908256880733945\n",
      "Basic Random Forest Classifier (max depth None) with n_estimators 1000 model accuracy: 0.9333333333333333, F1 Score: 0.908256880733945\n",
      "Basic Random Forest Classifier (max depth None) with n_estimators 5000 model accuracy: 0.9333333333333333, F1 Score: 0.908256880733945\n",
      "Basic Random Forest Classifier (max depth None) with n_estimators 10000 model accuracy: 0.9333333333333333, F1 Score: 0.908256880733945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "max_depth_list = [8, 9, 10, None]\n",
    "n_estimators_list = [2, 5, 10, 50, 100, 500, 1000, 5000, 10000]\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "    for n_estimator in n_estimators_list:\n",
    "        rfc = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth, n_jobs=-1, random_state=1)\n",
    "        rfc.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = rfc.predict(X_test)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        print(f\"Basic Random Forest Classifier (max depth {max_depth}) with n_estimators {n_estimator} model accuracy: {acc}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7712a16-e9f2-41c8-ba95-01ce7c44d0c4",
   "metadata": {},
   "source": [
    "## Random Forest Classifier Results (Non-K Fold)\n",
    "\n",
    "| Num Estimators | Max Depth | Accuracy | F1    |  | Num Estimators | Max Depth | Accuracy | F1    |  | Num Estimators | Max Depth | Accuracy | F1    |  | Num Estimators | Max Depth | Accuracy | F1    |\n",
    "| -------------- | --------- | -------- | ----- | ----- | -------------- | --------- | -------- | ----- | ----- | -------------- | --------- | -------- | ----- | ----- | -------------- | --------- | -------- | ----- |\n",
    "| 2              | 8         | 0.84     | 0.762 |  | 2              | 9         | 0.803    | 0.712 |  | 2              | 10        | 0.827    | 0.74  |  | 2              | None      | 0.813    | 0.682 |\n",
    "| 5              | 8         | 0.877    | 0.821 |  | 5              | 9         | 0.89     | 0.847 |  | 5              | 10        | 0.89     | 0.847 |  | 5              | None      | 0.887    | 0.833 |\n",
    "| 10             | 8         | 0.89     | 0.837 |  | 10             | 9         | 0.913    | 0.877 |  | 10             | 10        | 0.913    | 0.877 |  | 10             | None      | 0.897    | 0.849 |\n",
    "| 50             | 8         | 0.913    | 0.875 |  | 50             | 9         | 0.927    | 0.897 |  | 50             | 10        | 0.937    | 0.912 |  | 50             | None      | 0.92     | 0.888 |\n",
    "| 100            | 8         | 0.933    | 0.907 |  | 100            | 9         | 0.933    | 0.907 |  | 100            | 10        | 0.94     | 0.917 |  | 100            | None      | 0.937    | 0.912 |\n",
    "| 500            | 8         | 0.927    | 0.897 |  | 500            | 9         | 0.937    | 0.912 |  | 500            | 10        | 0.937    | 0.912 |  | 500            | None      | 0.933    | 0.908 |\n",
    "| 1000           | 8         | 0.923    | 0.891 |  | 1000           | 9         | 0.94     | 0.917 |  | 1000           | 10        | 0.937    | 0.912 |  | 1000           | None      | 0.933    | 0.908 |\n",
    "| 5000           | 8         | 0.927    | 0.896 |  | 5000           | 9         | 0.933    | 0.907 |  | 5000           | 10        | 0.937    | 0.912 |  | 5000           | None      | 0.933    | 0.908 |\n",
    "| 10000          | 8         | 0.927    | 0.896 |  | 10000          | 9         | 0.933    | 0.907 |  | 10000          | 10        | 0.937    | 0.912 |  | 10000          | None      | 0.933    | 0.908 |\n",
    "\n",
    "The best values range from 50 to 1000 estimators, and a max depth of 9 and 10. We will use 5-Fold validation to get a better mean accuracy and mean F1 score of all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a740f99c-234c-44e9-a81b-eca3e1e1cece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Random Forest Classifier (max depth 9) with n_estimators 50 model accuracy: 0.9153333333333332, F1 Score: 0.8762190231522409\n",
      "5-Fold Random Forest Classifier (max depth 9) with n_estimators 100 model accuracy: 0.9193333333333333, F1 Score: 0.883273507919853\n",
      "5-Fold Random Forest Classifier (max depth 9) with n_estimators 500 model accuracy: 0.9206666666666667, F1 Score: 0.8853504244201919\n",
      "5-Fold Random Forest Classifier (max depth 9) with n_estimators 1000 model accuracy: 0.9186666666666667, F1 Score: 0.8820235523892478\n",
      "5-Fold Random Forest Classifier (max depth 10) with n_estimators 50 model accuracy: 0.9179999999999999, F1 Score: 0.8811959529485304\n",
      "5-Fold Random Forest Classifier (max depth 10) with n_estimators 100 model accuracy: 0.9213333333333333, F1 Score: 0.8854519128127644\n",
      "5-Fold Random Forest Classifier (max depth 10) with n_estimators 500 model accuracy: 0.9233333333333335, F1 Score: 0.8888772441293451\n",
      "5-Fold Random Forest Classifier (max depth 10) with n_estimators 1000 model accuracy: 0.9213333333333333, F1 Score: 0.8863637532959879\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "\n",
    "max_depth_list = [9, 10]\n",
    "n_estimators_list = [50, 100, 500, 1000]\n",
    "    \n",
    "for max_depth in max_depth_list:\n",
    "    for n_estimator in n_estimators_list:\n",
    "        acc_mean = 0.0\n",
    "        f1_mean = 0.0\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(X, y)):                \n",
    "            rfc = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth, n_jobs=-1, random_state=1)\n",
    "            rfc.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "\n",
    "            y_pred = rfc.predict(X.iloc[test_index])\n",
    "            \n",
    "            acc_mean += accuracy_score(y.iloc[test_index], y_pred)\n",
    "            f1_mean += f1_score(y.iloc[test_index], y_pred)\n",
    "\n",
    "        print(f\"{n_splits}-Fold Random Forest Classifier (max depth {max_depth}) with n_estimators {n_estimator} model accuracy: {acc_mean/n_splits}, F1 Score: {f1_mean/n_splits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bbc586-afb9-4c6a-aa5b-e1a19c65b6b5",
   "metadata": {},
   "source": [
    "## Random Forest Classifier Results (5-Fold)\n",
    "\n",
    "| Num Estimators | Max Depth | Accuracy | F1    | -- | Max Depth | Accuracy | F1    |\n",
    "| -------------- | --------- | -------- | ----- | -- | --------- | -------- | ----- |\n",
    "| 50             | 9         | 0.915    | 0.876 |    | 10        | 0.918    | 0.881 |\n",
    "| 100            | 9         | 0.919    | 0.883 |    | 10        | 0.921    | 0.885 |\n",
    "| 500            | 9         | 0.921    | 0.885 |    | 10        | 0.923    | 0.889 |\n",
    "| 1000           | 9         | 0.919    | 0.882 |    | 10        | 0.921    | 0.886 |\n",
    "\n",
    "The model performs best at 500 estimators, at a max depth of 10, when evaluated with 5-fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e4c49a-d503-4bc2-a26c-683e2c2f1076",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Key Findings/Analysis\n",
    "\n",
    "From the Logistic Regression model, the best predictors of cancer are the Cancer History, smoking, and age columns. This ties in with common wisdom that people who have had cancer previously are more likely to get cancer in the future, and that smoking increases the risks of cancer, and older people are more likely to have cancer than younger people. Notably, women are only slightly likely to have cancer than men, even though the data is skewed to having more a larger proportion of women with cancer, as compared to men. It has less of an impact than Genetic Risk and Physical Activity, which have a small impact on the Logistic Regression model.\n",
    "\n",
    "Both the Random Forest Classifier and the Decision Tree Classifiers have better performance in terms of accuracy and F1 score as compared to the Logistic Regression. In terms of this dataset, using a Random Forest Classifier of around 500 estimators, with a max depth of 10, will provide close to the best accuracy and F1 score on the validation dataset.\n",
    "\n",
    "It is recommended that the Random Forest Classifier be used.\n",
    "\n",
    "## Future Work\n",
    "Deep Learning models may be able to provide a better model of predicting cancer. Other methods to be tested include oversampling, undersampling, and dropping columns such as gender to see if the predictive ability of the model improves with less noisy data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
